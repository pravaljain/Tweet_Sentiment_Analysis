#!/usr/bin/env python# -*- coding: utf-8 -*-import csvimport openpyxl as pximport xlrdimport reimport nltkimport numpy as npimport pandas as pdimport timeitfrom nltk.stem import PorterStemmerfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.ensemble import VotingClassifierfrom sklearn import linear_modelfrom sklearn import svmfrom sklearn.neural_network import MLPClassifierfrom sklearn.utils import shufflefrom sklearn import metricsfrom sklearn.naive_bayes import BernoulliNBfrom sklearn.model_selection import train_test_splitfrom sklearn import metricsfrom nltk.stem.snowball import SnowballStemmer# from sklearn import ensemble# from sklearn import datasets# from sklearn.utils import shufflefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import AdaBoostClassifier# from sklearn.metrics import mean_squared_errorfrom sklearn.naive_bayes import GaussianNBfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.ensemble import GradientBoostingClassifierstopwords=['a','am','at','are','ah','an','as','and','aw','be','been','by','between','during','each','few','for','from',           'general','generally','he','she','her','him','they','is','be','himself','herself','in','if',           'is','just','here','it','its','me','my','myself','on','or','of','p','so','up','us','was','we'            ,'who','whom','why','with','where','when','what','your','yours','a','b','c','d',           'e','f','g','h','i','j','k','l','m','n','o','q','r','s','t','u','v','w','x','y','z']stemmer = SnowballStemmer("english")def preprocess(file_name, sheetname, csvoutput):    start_time = timeit.default_timer()    Excelsheet = px.load_workbook(file_name)    sheet = Excelsheet.get_sheet_by_name(name=sheetname)    csv_file = open(csvoutput, 'w', newline='')    wr = csv.writer(csv_file, delimiter=',', quoting=csv.QUOTE_ALL)    counter=0    data2=[];'''    # acro_dic dictionary    acro_dic = {}    # Open the file in Universal mode    with open('acrynom.csv', 'rU') as f:        # Get the CSV reader and skip header        reader = csv.reader(f)        next(reader)        for row in reader:            # First column is the key, the rest is value            acro_dic[row[0]] = row[1:]    # emo_dic dictionary    emo_dic = {}    # Open the file in Universal mode    with open('Emoticons.csv', 'rU') as g:        # Get the CSV reader and skip header        reader = csv.reader(g)        next(reader)        for row in reader:            # First column is the key, the rest is value            emo_dic[row[0]] = row[1:]'''    for entry in sheet['A3':'E' + str(sheet.max_row)]:        if entry[4].value in [1, -1, 0]:            tweet=str(entry[3].value)            tweet = re.sub(' +', ' ', tweet)            '''            # ACRONYM            for line in acro_dic.keys():                sptweet = tweet.split(" ")                pp_tweet = [];                for spword in sptweet:                    if spword == line:                        pp_tweet.append(str(acro_dic[line]))                    else:                        pp_tweet.append(spword);                tweet = " ".join([pp_tweet[kw] for kw in range(len(pp_tweet))])            # EMOTICONS            for line in emo_dic.keys():                # print("line:", line)                sptweet = tweet.split(" ")                pp_tweet = [];                for spword in sptweet:                    if spword == line:                        #print("keys:", list(emo_dic.keys()))                        #print("value:", list(emo_dic[line]))                        pp_tweet.append(str(emo_dic[line]))                    else:                        pp_tweet.append(spword);                tweet = " ".join([pp_tweet[kw] for kw in range(len(pp_tweet))])'''            tweet = re.sub(r'<[^>]+>', '', tweet).strip()  # for html tags            tweet = tweet.strip("#")  # removes only hash            tweet = tweet.lower().strip()            tweet = re.sub(r'^https?:\/\/.*[\r\n]*', '', tweet, flags=re.MULTILINE)            tweet = re.sub(                r'(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:\'".,<>?«»“”‘’]))',                '', tweet)            # tweet = re.sub(r'#([^\s]+)', r'\1', tweet).strip()            tweet = re.sub(r'(@[^\s]+)', '', tweet).strip()            tweet = tweet.replace("\"", "")            tweet = tweet.replace('...', '')            tweet = tweet.replace('\n', '')            pattern = re.compile(r"(.)\1{1,}", re.DOTALL)            tweet = pattern.sub(r"\1\1", tweet)            tweet = tweet.replace('…', '');            tweet = re.sub(r'[^\w\s]', '', tweet)            # tweet=re.sub(r'\w*\d\w*', '', tweet).strip() #remove words that have numbers            tweet = " ".join([stemmer.stem(kw) for kw in tweet.split(" ")])            sptweet=tweet.split(" ")            pp_tweet=[];            for spword in sptweet:                if spword not in stopwords:                    pp_tweet.append(spword)            tweet = " ".join([pp_tweet[kw] for kw in range(len(pp_tweet))])            if entry[4].value!=2 and entry[4].value!="!!!!" and entry[4].value!="irrelevant" and entry[4].value!=" " and entry[4].value!="" and entry[4].value is not None:                counter=counter+1;                tweets = [tweet, entry[4].value]                data2.append(tweets)            wr.writerow(tweets)    csv_file.close()    data=np.array(data2)    data_labels = data[:, 1]    data_tweets = data[:, 0]    #-------------------PREPROCESS TEST DATA    data2 = [];    #Excelsheet = px.load_workbook('testing-Obama-Romney-tweets-3labels.xlsx')    Excelsheet = px.load_workbook('testupdated.xlsx')    sheet = Excelsheet.get_sheet_by_name(name=sheetname)    csv_file = open('testd.csv', 'w', newline='')    wr = csv.writer(csv_file, delimiter=',', quoting=csv.QUOTE_ALL)    counter=0    data2=[];'''    # acro_dic dictionary    acro_dic = {}    # Open the file in Universal mode    with open('acrynom.csv', 'rU') as f:        # Get the CSV reader and skip header        reader = csv.reader(f)        next(reader)        for row in reader:            # First column is the key, the rest is value            acro_dic[row[0]] = row[1:]    # emo_dic dictionary    emo_dic = {}    # Open the file in Universal mode    with open('Emoticons.csv', 'rU') as g:        # Get the CSV reader and skip header        reader = csv.reader(g)        next(reader)        for row in reader:            # First column is the key, the rest is value            emo_dic[row[0]] = row[1:]'''    for entry in sheet['A1':'B' + str(sheet.max_row)]:        if entry[1].value in [1, -1, 0]:            tweet=str(entry[0].value)            tweet = re.sub(' +', ' ', tweet)            '''            # ACRONYM            for line in acro_dic.keys():                sptweet = tweet.split(" ")                pp_tweet = [];                for spword in sptweet:                    if spword == line:                        pp_tweet.append(str(acro_dic[line]))                    else:                        pp_tweet.append(spword);                tweet = " ".join([pp_tweet[kw] for kw in range(len(pp_tweet))])            # EMOTICONS            for line in emo_dic.keys():                # print("line:", line)                sptweet = tweet.split(" ")                pp_tweet = [];                for spword in sptweet:                    if spword == line:                        #print("keys:", list(emo_dic.keys()))                        #print("value:", list(emo_dic[line]))                        pp_tweet.append(str(emo_dic[line]))                    else:                        pp_tweet.append(spword);                tweet = " ".join([pp_tweet[kw] for kw in range(len(pp_tweet))])'''            tweet = re.sub(r'<[^>]+>', '', tweet).strip()  # for html tags            tweet = tweet.strip("#")  # removes only hash            tweet = tweet.lower().strip()            tweet = re.sub(r'^https?:\/\/.*[\r\n]*', '', tweet, flags=re.MULTILINE)            tweet = re.sub(                r'(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:\'".,<>?«»“”‘’]))',                '', tweet)            # tweet = re.sub(r'#([^\s]+)', r'\1', tweet).strip()            tweet = re.sub(r'(@[^\s]+)', '', tweet).strip()            tweet = tweet.replace("\"", "")            tweet = tweet.replace('...', '')            tweet = tweet.replace('\n', '')            pattern = re.compile(r"(.)\1{1,}", re.DOTALL)            tweet = pattern.sub(r"\1\1", tweet)            tweet = tweet.replace('…', '');            tweet = re.sub(r'[^\w\s]', '', tweet)            # tweet=re.sub(r'\w*\d\w*', '', tweet).strip() #remove words that have numbers            tweet = " ".join([stemmer.stem(kw) for kw in tweet.split(" ")])            sptweet=tweet.split(" ")            pp_tweet=[];            for spword in sptweet:                if spword not in stopwords:                    pp_tweet.append(spword)            tweet = " ".join([pp_tweet[kw] for kw in range(len(pp_tweet))])            if entry[1].value!=2 and entry[1].value!="!!!!" and entry[1].value!="irrelevant" and entry[1].value!=" " and entry[1].value!="" and entry[1].value is not None:                counter=counter+1;                tweets = [tweet, entry[1].value]                data2.append(tweets)            wr.writerow(tweets)    csv_file.close()    ########PREPROCESS TEST DATA FINISH    data_test=np.array(data2)    data_labels_test = data_test[:, 1]    data_tweets_test = data_test[:, 0]    accuracy_all_iter=[]    precision_all=[]    recall_all= []    f1_score_all=[]    if 1==1:        np.random.shuffle(data)        data_labels = data[:, 1]        data_tweets = data[:, 0]        X_train=data_tweets;        y_train=data_labels        X_test = data_tweets_test;        y_test=data_labels_test        #print('----Iteration '+str(1)+'----')        tfidf_v = TfidfVectorizer(lowercase=True)        tfidf_train = tfidf_v.fit_transform(X_train)        tfidf_test = tfidf_v.transform(X_test)        # ::::::::CLASSIFIERS::::::::::        # ---K Nearest Neibour        knn_clf = KNeighborsClassifier(n_neighbors=1)        # ---NEURAL NETWORKS        nn_clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes = (50, 20), random_state = 1)        # ---NAIVE BAYES CLASSIFIER        naiveb_clf = MultinomialNB(alpha=1.5)        clf_gbc =  AdaBoostClassifier(MultinomialNB(alpha=1),algorithm="SAMME.R",n_estimators=500)        #GN=GaussianNB()        # ---REGRESSION, LOGISTIC        lr_clf = linear_model.LogisticRegression(C=1e5)        # ---Stochastic Gradient Descent classifier        gradient_clf = linear_model.SGDClassifier(loss='log')        # ---SUPPORT VECTOR MACHINE        supportvector_clf = svm.SVC(kernel='rbf', C=1.5, gamma=1.2)        # Extreme Gradient Boosting        # params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,'learning_rate': 0.01, 'loss': 'ls'}        # GB_clf = ensemble.GradientBoostingRegressor(**params)        # Combining Classifiers        # combine_clf = VotingClassifier(estimators=[('Naivebayes', naiveb_clf), ('SupportVector', supportvector_clf), ('StochGradient', gradient_clf)], voting='hard')        combine_clf = VotingClassifier(estimators=[('SVM', supportvector_clf)], voting='hard')        combine_clf = supportvector_clf.fit(tfidf_train, y_train)        combination_p = combine_clf.predict(tfidf_test)        # Accuracy of the aggregate classifiers        combination_accuracy = metrics.accuracy_score(y_test, combination_p)        print ('Accuracy:', combination_accuracy)        labels_order = ['-1', '1']        accuracy_all_iter.append(combination_accuracy)        precision = metrics.precision_score(y_test, combination_p, average=None,labels=labels_order)        precision_all.append(precision);        recall = metrics.recall_score(y_test, combination_p, average=None,labels=labels_order)        recall_all.append(recall);        f1 = metrics.f1_score(y_test, combination_p, average=None,labels=labels_order)        f1_score_all.append(f1);        '''        confusion_metrics = metrics.confusion_matrix(y_test, combination_p,labels=labels_order)        print('Precision:', precision)        print('Recall:', recall)        print('F1 Score:', f1)        print ('Confusion Matrix:\n',confusion_metrics)        '''    # print ('K-FOLD Average accuracy of', str(sheet2),' :',np.mean(np.array(accuracy_all_iter)))    elapsed = timeit.default_timer() - start_time    precision = []    precision = np.mean(np.array(precision_all), axis=0)    recall = []    recall = np.mean(np.array(recall_all), axis=0)    fscore = []    fscore = np.mean(np.array(f1_score_all), axis=0)    print('-------POSITIVE CLASS-------')    print('Precision for', str(sheet1), ' :', precision[0])    print('Recall for', str(sheet1), ' :', recall[0])    print('F1 for', str(sheet1), ' :', fscore[0])    print('')    print('-------NEGATIVE CLASS-------')    print('Precision for', str(sheet1), ' :', precision[1])    print('Recall for', str(sheet1), ' :', recall[1])    print('F1 for', str(sheet1), ' :', fscore[1])    print('')    elapsed = timeit.default_timer() - start_time    print("The Total run time of the script was ", elapsed, " seconds.")# -----------RUN SETTINGSfile_name='training-Obama-Romney-tweets.xlsx'sheet1 = 'Obama'#sheet1 = 'Romney'# preprocess(file_name, sheet2, str(sheet2)+'.csv')   # romneypreprocess(file_name, sheet1, str(sheet1)+'.csv')   # Obamma#-----------RUN SETTINGS